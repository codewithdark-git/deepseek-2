{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distilling the Mistral Small Model from Deepseek R1 on TPU \n",
    "\n",
    "In this notebook we will use the Wikitext-2 dataset from Hugging Face Datasets to perform distillation.\n",
    "**Steps:**\n",
    "1. Setup the TPU environment.\n",
    "2. Load the teacher (Deepseek R1) and student (Mistral Small) models along with the tokenizer.\n",
    "3. Load and tokenize the Wikitext-2 dataset.\n",
    "4. Define a collator for the DataLoader.\n",
    "5. Define a distillation loss function (KL divergence + cross-entropy).\n",
    "6. Run the training loop on TPU using PyTorch/XLA’s distributed tools.\n",
    "7. Save the distilled student model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required Packages:**\n",
    "\n",
    "- `torch`  \n",
    "- `torch_xla`  \n",
    "- `transformers`  \n",
    "- `datasets`  \n",
    "- `huggingface_hub`\n",
    "\n",
    "You can install them via pip:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio\n",
    "pip install torch_xla  # Make sure you use the appropriate version for your TPU environment\n",
    "pip install transformers datasets huggingface_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:25:34.333500Z",
     "iopub.status.busy": "2025-02-02T14:25:34.333223Z",
     "iopub.status.idle": "2025-02-02T14:25:50.040633Z",
     "shell.execute_reply": "2025-02-02T14:25:50.039746Z",
     "shell.execute_reply.started": "2025-02-02T14:25:34.333477Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "# !pip install torch_xla\n",
    "!pip install transformers==4.44.2 datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:25:50.041989Z",
     "iopub.status.busy": "2025-02-02T14:25:50.041670Z",
     "iopub.status.idle": "2025-02-02T14:25:53.920016Z",
     "shell.execute_reply": "2025-02-02T14:25:53.919372Z",
     "shell.execute_reply.started": "2025-02-02T14:25:50.041958Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Import Hugging Face Transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Import PyTorch XLA modules for TPU support\n",
    "# import torch_xla\n",
    "# import torch_xla.core.xla_model as xm\n",
    "# import torch_xla.distributed.parallel_loader as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup TPU Environment\n",
    "\n",
    "This cell initializes the TPU device. In Google Colab, make sure you select TPU as the accelerator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:25:53.921163Z",
     "iopub.status.busy": "2025-02-02T14:25:53.920763Z",
     "iopub.status.idle": "2025-02-02T14:25:53.987521Z",
     "shell.execute_reply": "2025-02-02T14:25:53.986500Z",
     "shell.execute_reply.started": "2025-02-02T14:25:53.921140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# device = xm.xla_device() \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Models and Tokenizer\n",
    "\n",
    "**Note:** Replace `\"deepseek/deepseek-r1\"` and `\"meta-llama/Meta-Llama-3-8B\"` with the model identifiers or paths which you Want to distill.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:25:53.989895Z",
     "iopub.status.busy": "2025-02-02T14:25:53.989634Z",
     "iopub.status.idle": "2025-02-02T14:25:54.685130Z",
     "shell.execute_reply": "2025-02-02T14:25:54.684082Z",
     "shell.execute_reply.started": "2025-02-02T14:25:53.989874Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token Add here the hf token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:25:54.687394Z",
     "iopub.status.busy": "2025-02-02T14:25:54.687093Z",
     "iopub.status.idle": "2025-02-02T14:25:54.690981Z",
     "shell.execute_reply": "2025-02-02T14:25:54.690032Z",
     "shell.execute_reply.started": "2025-02-02T14:25:54.687369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model names\n",
    "teacher_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "student_model_name = \"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:25:54.692316Z",
     "iopub.status.busy": "2025-02-02T14:25:54.692002Z",
     "iopub.status.idle": "2025-02-02T14:28:06.273674Z",
     "shell.execute_reply": "2025-02-02T14:28:06.272925Z",
     "shell.execute_reply.started": "2025-02-02T14:25:54.692284Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n",
    "\n",
    "# Ensure tokenizer has a pad token\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Load teacher model\n",
    "teacher = AutoModelForCausalLM.from_pretrained(teacher_model_name, trust_remote_code=True)\n",
    "teacher = teacher.to(dtype=torch.float16)\n",
    "teacher.resize_token_embeddings(len(tokenizer))\n",
    "teacher.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:28:06.274975Z",
     "iopub.status.busy": "2025-02-02T14:28:06.274514Z",
     "iopub.status.idle": "2025-02-02T14:29:34.829134Z",
     "shell.execute_reply": "2025-02-02T14:29:34.828259Z",
     "shell.execute_reply.started": "2025-02-02T14:28:06.274948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load student model\n",
    "student = AutoModelForCausalLM.from_pretrained(student_model_name,trust_remote_code=True)\n",
    "student = student.to(dtype=torch.float16)\n",
    "student.resize_token_embeddings(len(tokenizer))\n",
    "student.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Tokenize the Wikitext-2 Dataset\n",
    "\n",
    "Here we load the “wikitext-2-raw-v1” split (training portion) and tokenize the text using the loaded tokenizer.\n",
    "\n",
    "We use a maximum sequence length of 128 tokens. Adjust as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define a Data Collator\n",
    "\n",
    "A collator is used to batch the data correctly. This collator pads sequences dynamically to the longest\n",
    "sequence in the batch. We also ensure that our `input_ids` and `attention_mask` tensors are padded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Distillation Loss and Helper Functions\n",
    "\n",
    "We combine two losses:\n",
    "\n",
    " - **KL Divergence Loss:** Between the softened output distributions (using a temperature) of teacher and student.\n",
    " - **Cross Entropy Loss:** Using ground truth tokens.\n",
    "\n",
    "The final loss is a weighted sum of both. Adjust the temperature and alpha as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:29:34.830398Z",
     "iopub.status.busy": "2025-02-02T14:29:34.830075Z",
     "iopub.status.idle": "2025-02-02T14:29:34.834303Z",
     "shell.execute_reply": "2025-02-02T14:29:34.833596Z",
     "shell.execute_reply.started": "2025-02-02T14:29:34.830365Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loss functions and hyperparameters\n",
    "temperature = 2.0\n",
    "alpha = 0.7\n",
    "kl_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "ce_loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-02-02T14:29:34.835428Z",
     "iopub.status.busy": "2025-02-02T14:29:34.835132Z",
     "iopub.status.idle": "2025-02-02T14:29:38.269350Z",
     "shell.execute_reply": "2025-02-02T14:29:38.268490Z",
     "shell.execute_reply.started": "2025-02-02T14:29:34.835396Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:29:38.270861Z",
     "iopub.status.busy": "2025-02-02T14:29:38.270497Z",
     "iopub.status.idle": "2025-02-02T14:29:59.366733Z",
     "shell.execute_reply": "2025-02-02T14:29:59.365763Z",
     "shell.execute_reply.started": "2025-02-02T14:29:38.270815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",  # Pad to max_length\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "raw_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "# Data collator and DataLoader\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Disable MLM for causal language modeling\n",
    "    pad_to_multiple_of=8  # Optional: Pad to a multiple of 8 for better performance\n",
    ")\n",
    "\n",
    "batch_size = 2\n",
    "data_loader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:29:59.368487Z",
     "iopub.status.busy": "2025-02-02T14:29:59.367760Z",
     "iopub.status.idle": "2025-02-02T14:29:59.375228Z",
     "shell.execute_reply": "2025-02-02T14:29:59.374500Z",
     "shell.execute_reply.started": "2025-02-02T14:29:59.368433Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def distillation_loss(\n",
    "    student_logits, \n",
    "    teacher_logits, \n",
    "    target_ids, \n",
    "    temperature=2.0, \n",
    "    alpha=0.5,\n",
    "    kl_loss_fn=nn.KLDivLoss(reduction=\"batchmean\"), \n",
    "    ce_loss_fn=nn.CrossEntropyLoss(ignore_index=-100)\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the distillation loss as a weighted sum of KL divergence loss (between teacher and student outputs)\n",
    "    and cross-entropy loss (using target_ids for next-token prediction).\n",
    "\n",
    "    Args:\n",
    "        student_logits (Tensor): Logits from the student model with shape [batch, seq_len, vocab_size].\n",
    "        teacher_logits (Tensor): Logits from the teacher model with shape [batch, seq_len, vocab_size].\n",
    "        target_ids (Tensor): Ground-truth token IDs with shape [batch, seq_len].\n",
    "        temperature (float): Temperature for smoothing the logits.\n",
    "        alpha (float): Weight for the distillation (KL) loss; (1 - alpha) is used for the CE loss.\n",
    "        kl_loss_fn (nn.Module): KL divergence loss function.\n",
    "        ce_loss_fn (nn.Module): Cross-entropy loss function.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The computed combined loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Apply temperature scaling to logits\n",
    "    student_logits_temp = student_logits / temperature\n",
    "    teacher_logits_temp = teacher_logits / temperature\n",
    "\n",
    "    # 2. Compute softened probabilities/log-probabilities\n",
    "    student_log_probs = F.log_softmax(student_logits_temp, dim=-1)\n",
    "    teacher_probs = F.softmax(teacher_logits_temp, dim=-1)\n",
    "    \n",
    "    # 3. Compute KL divergence loss\n",
    "    #    Reshape to 2D tensors with shape [batch * seq_len, vocab_size]\n",
    "    loss_kl = kl_loss_fn(\n",
    "        student_log_probs.view(-1, student_log_probs.size(-1)),\n",
    "        teacher_probs.view(-1, teacher_probs.size(-1))\n",
    "    ) * (temperature ** 2)\n",
    "    \n",
    "    # 4. Prepare logits and labels for the next-token prediction cross-entropy loss.\n",
    "    #    We shift the student logits and target_ids so that each prediction is compared to the next token.\n",
    "    shift_logits = student_logits[..., :-1, :].contiguous()  # shape: [batch, seq_len-1, vocab_size]\n",
    "    shift_labels = target_ids[..., 1:].contiguous()           # shape: [batch, seq_len-1]\n",
    "    \n",
    "    # 5. (Optional) Check that the labels are within the valid range.\n",
    "    vocab_size = student_logits.size(-1)\n",
    "    if torch.any(shift_labels < 0) or torch.any(shift_labels >= vocab_size):\n",
    "        raise ValueError(f\"Target ids contain values outside the valid range [0, {vocab_size-1}].\")\n",
    "    \n",
    "    # 6. Compute the cross-entropy loss for next-token prediction.\n",
    "    loss_ce = ce_loss_fn(\n",
    "        shift_logits.view(-1, vocab_size),  # flatten logits to [batch * (seq_len-1), vocab_size]\n",
    "        shift_labels.view(-1)               # flatten labels to [batch * (seq_len-1)]\n",
    "    )\n",
    "    \n",
    "    # 7. Combine losses with weighting\n",
    "    loss = alpha * loss_kl + (1 - alpha) * loss_ce\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop on TPU\n",
    "\n",
    "We use PyTorch/XLA’s `ParallelLoader` to feed data to the TPU cores. In a multi-core TPU setup,\n",
    "you might launch the training using `xmp.spawn()`. Here, we run a single-process loop.\n",
    "\n",
    "The loop performs the following for each batch:\n",
    " - Moves the batch to the TPU device.\n",
    " - Computes the teacher’s outputs (without gradient computation).\n",
    " - Computes the student’s outputs.\n",
    " - Computes the distillation loss.\n",
    " - Backpropagates and updates the student model using the TPU-optimized optimizer step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:29:59.376584Z",
     "iopub.status.busy": "2025-02-02T14:29:59.376251Z",
     "iopub.status.idle": "2025-02-02T14:30:01.109704Z",
     "shell.execute_reply": "2025-02-02T14:30:01.108988Z",
     "shell.execute_reply.started": "2025-02-02T14:29:59.376553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "learning_rate = 2e-5\n",
    "optimizer = optim.AdamW(student.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T14:30:01.112472Z",
     "iopub.status.busy": "2025-02-02T14:30:01.112232Z",
     "iopub.status.idle": "2025-02-02T14:30:01.119213Z",
     "shell.execute_reply": "2025-02-02T14:30:01.118459Z",
     "shell.execute_reply.started": "2025-02-02T14:30:01.112435Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "accumulation_steps = 4\n",
    "\n",
    "def train_loop_fn(loader, epoch, student, teacher, optimizer, device):\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_steps = len(loader)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(**batch)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "        \n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "            student_outputs = student(**batch)\n",
    "            student_logits = student_outputs.logits\n",
    "            loss = distillation_loss(student_logits, teacher_logits, batch['input_ids'])\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if step % 500 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step}/{total_steps} | Loss: {loss.item()*accumulation_steps:.4f}\")\n",
    "\n",
    "    average_loss = total_loss / total_steps\n",
    "    print(f\"Epoch {epoch} completed. Average Loss: {average_loss:.4f}\")\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Training with PyTorch/XLA Parallel Loader\n",
    "\n",
    "We wrap our DataLoader with a ParallelLoader so that data is distributed to the TPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-02T14:30:01.120807Z",
     "iopub.status.busy": "2025-02-02T14:30:01.120500Z",
     "iopub.status.idle": "2025-02-02T14:30:04.505682Z",
     "shell.execute_reply": "2025-02-02T14:30:04.504529Z",
     "shell.execute_reply.started": "2025-02-02T14:30:01.120774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = train_loop_fn(data_loader, epoch, student, teacher, optimizer, device)\n",
    "    print(f\"Epoch {epoch} completed. Average Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the Distilled Student Model\n",
    "\n",
    "Finally, we save the distilled student model and tokenizer. Adjust the saving path as necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-02T14:30:04.506251Z",
     "iopub.status.idle": "2025-02-02T14:30:04.506569Z",
     "shell.execute_reply": "2025-02-02T14:30:04.506419Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "save_directory = \"./R!--distill--Qwen2.5\"\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "student.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(\"Distilled model saved to:\", save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Push the Distilled Model to the Hugging Face Hub\n",
    "\n",
    "Before pushing, you can log in interactively if needed. Run the following cell and follow the instructions:\n",
    "\n",
    "```python\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "```\n",
    "\n",
    "Once authenticated, push the model and tokenizer to your repository.\n",
    "\n",
    "**Important:** Replace `\"your-username/R!--distill--llama\"` with your desired repository name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-02T14:30:04.507480Z",
     "iopub.status.idle": "2025-02-02T14:30:04.507758Z",
     "shell.execute_reply": "2025-02-02T14:30:04.507652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Push the student model\n",
    "student.push_to_hub(\"codewithdark/R!-distill-Qwen2.5\")\n",
    "# Push the tokenizer\n",
    "tokenizer.push_to_hub(\"codewithdark/R!-distill-Qwen2.5\")\n",
    "\n",
    "print(\"Model and tokenizer have been pushed to the Hugging Face Hub!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30841,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
